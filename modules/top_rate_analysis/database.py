#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
ê°œì„ ëœ AI ë¶„ì„ ë°ì´í„°ë² ì´ìŠ¤ ìŠ¤í‚¤ë§ˆ
ìƒì„¸ ë¶„ì„ ì •ë³´ë¥¼ ìœ„í•œ ì»¬ëŸ¼ ì¶”ê°€
"""

import pymysql
import logging
import json
from datetime import datetime
from typing import List, Dict, Optional, Any
import os
from dotenv import load_dotenv

load_dotenv()


class EnhancedTopRateDatabase:
    """ê°œì„ ëœ ë“±ë½ìœ¨ìƒìœ„ë¶„ì„ ì „ìš© ë°ì´í„°ë² ì´ìŠ¤ í´ë˜ìŠ¤"""

    def __init__(self):
        self.db_config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': int(os.getenv('DB_PORT', 3306)),
            'user': os.getenv('DB_USER'),
            'password': os.getenv('DB_PASSWORD'),
            'charset': 'utf8mb4',
            'autocommit': True
        }
        self.crawling_db = 'crawling_db'

    def get_connection(self, database: str = None) -> pymysql.Connection:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë°˜í™˜"""
        config = self.db_config.copy()
        if database:
            config['database'] = database
        return pymysql.connect(**config)

    def setup_enhanced_ai_analysis_table(self, date: str) -> str:
        """ê°œì„ ëœ AI ë¶„ì„ í…Œì´ë¸” ì„¤ì • (ìƒì„¸ ì»¬ëŸ¼ ì¶”ê°€)"""
        try:
            table_name = f"ai_analysis_{date.replace('-', '')}"

            connection = self.get_connection(self.crawling_db)
            cursor = connection.cursor()

            # ê¸°ì¡´ í…Œì´ë¸” ì‚­ì œ
            cursor.execute(f"DROP TABLE IF EXISTS {table_name}")
            logging.info(f"ğŸ—‘ï¸ ê¸°ì¡´ {table_name} í…Œì´ë¸” ì‚­ì œ")

            # ê°œì„ ëœ í…Œì´ë¸” ìƒì„±
            create_table_sql = f"""
            CREATE TABLE {table_name} (
                id INT AUTO_INCREMENT PRIMARY KEY,

                -- ê¸°ë³¸ ì •ë³´
                stock_code VARCHAR(10) NOT NULL,
                stock_name VARCHAR(100) NOT NULL,
                primary_theme VARCHAR(100) NOT NULL,

                -- ë¶„ì„ ê²°ê³¼
                issue_type ENUM('THEME', 'INDIVIDUAL') NOT NULL,
                issue_category VARCHAR(50),
                ai_score INT DEFAULT 0,
                confidence_level DECIMAL(3,2) DEFAULT 0.50,
                investment_opinion ENUM('ê°•ë ¥ë§¤ìˆ˜', 'ë§¤ìˆ˜', 'ë³´ìœ ', 'ê´€ì‹¬', 'ê´€ë§', 'ë§¤ë„') DEFAULT 'ê´€ë§',

                -- ìƒì„¸ ì ìˆ˜ ë¶„í•´ (ì‹ ê·œ ì¶”ê°€)
                keyword_score INT DEFAULT 0,
                combo_bonus INT DEFAULT 0,
                market_score INT DEFAULT 0,
                sustainability_score INT DEFAULT 0,
                total_calculated_score INT DEFAULT 0,

                -- ìƒì„¸ ë¶„ì„ ì •ë³´ (ì‹ ê·œ ì¶”ê°€)
                found_keywords JSON,
                found_categories JSON,
                analysis_summary TEXT,

                -- ê¸°ì¡´ í•„ë“œ
                key_factors JSON NOT NULL,
                news_summary TEXT,
                ai_reasoning TEXT,

                -- ë©”íƒ€ ì •ë³´
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,

                -- ì¸ë±ìŠ¤
                INDEX idx_stock_code (stock_code),
                INDEX idx_issue_type (issue_type),
                INDEX idx_ai_score (ai_score DESC),
                INDEX idx_investment_opinion (investment_opinion),
                INDEX idx_total_score (total_calculated_score DESC),
                INDEX idx_keyword_score (keyword_score DESC)
            ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_unicode_ci
            """

            cursor.execute(create_table_sql)
            logging.info(f"âœ… ê°œì„ ëœ {table_name} í…Œì´ë¸” ìƒì„± ì™„ë£Œ")

            # í…Œì´ë¸” êµ¬ì¡° í™•ì¸ ë° ì¶œë ¥
            cursor.execute(f"DESCRIBE {table_name}")
            columns = cursor.fetchall()

            logging.info(f"\nğŸ“‹ {table_name} í…Œì´ë¸” êµ¬ì¡°:")
            for col in columns:
                col_name, col_type, null, key, default, extra = col
                logging.info(f"   {col_name}: {col_type} {extra if extra else ''}")

            cursor.close()
            connection.close()
            return table_name

        except Exception as e:
            logging.error(f"AI ë¶„ì„ í…Œì´ë¸” ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    def save_enhanced_ai_analysis(self, table_name: str, analysis_data: List[Dict]) -> bool:
        """ê°œì„ ëœ AI ë¶„ì„ ê²°ê³¼ ì €ì¥ (ìƒì„¸ ì •ë³´ í¬í•¨)"""
        try:
            connection = self.get_connection(self.crawling_db)
            cursor = connection.cursor()

            insert_sql = f"""
            INSERT INTO {table_name} 
            (stock_code, stock_name, primary_theme, issue_type, issue_category, 
             ai_score, confidence_level, investment_opinion,
             keyword_score, combo_bonus, market_score, sustainability_score, total_calculated_score,
             found_keywords, found_categories, analysis_summary,
             key_factors, news_summary, ai_reasoning)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            """

            success_count = 0
            for analysis in analysis_data:
                try:
                    cursor.execute(insert_sql, (
                        analysis['stock_code'],
                        analysis['stock_name'],
                        analysis['primary_theme'],
                        analysis['issue_type'],
                        analysis.get('issue_category', ''),
                        analysis['ai_score'],
                        analysis.get('confidence_level', 0.5),
                        analysis['investment_opinion'],

                        # ìƒì„¸ ì ìˆ˜ ì •ë³´
                        analysis.get('keyword_score', 0),
                        analysis.get('combo_bonus', 0),
                        analysis.get('market_score', 0),
                        analysis.get('sustainability_score', 0),
                        analysis.get('total_calculated_score', 0),

                        # ìƒì„¸ ë¶„ì„ ì •ë³´
                        json.dumps(analysis.get('found_keywords', []), ensure_ascii=False),
                        json.dumps(analysis.get('found_categories', []), ensure_ascii=False),
                        analysis.get('analysis_summary', ''),

                        # ê¸°ì¡´ í•„ë“œ
                        json.dumps(analysis.get('key_factors', []), ensure_ascii=False),
                        analysis.get('news_summary', ''),
                        analysis.get('ai_reasoning', '')
                    ))
                    success_count += 1

                except Exception as e:
                    logging.error(f"AI ë¶„ì„ ì €ì¥ ì‹¤íŒ¨ ({analysis.get('stock_name', 'Unknown')}): {e}")

            cursor.close()
            connection.close()

            logging.info(f"âœ… ê°œì„ ëœ AI ë¶„ì„ ì €ì¥ ì™„ë£Œ: {success_count}/{len(analysis_data)}ê°œ ì¢…ëª©")
            return True

        except Exception as e:
            logging.error(f"AI ë¶„ì„ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def get_enhanced_ai_analysis(self, date: str) -> List[Dict]:
        """ê°œì„ ëœ AI ë¶„ì„ ê²°ê³¼ ì¡°íšŒ (ìƒì„¸ ì •ë³´ í¬í•¨)"""
        try:
            table_name = f"ai_analysis_{date.replace('-', '')}"

            connection = self.get_connection(self.crawling_db)
            cursor = connection.cursor(pymysql.cursors.DictCursor)

            query = f"""
            SELECT 
                stock_code, stock_name, primary_theme, issue_type, issue_category,
                ai_score, confidence_level, investment_opinion,
                keyword_score, combo_bonus, market_score, sustainability_score, total_calculated_score,
                found_keywords, found_categories, analysis_summary,
                key_factors, news_summary, ai_reasoning,
                created_at
            FROM {table_name}
            ORDER BY ai_score DESC, total_calculated_score DESC
            """

            cursor.execute(query)
            result = cursor.fetchall()

            cursor.close()
            connection.close()

            return result if result else []

        except Exception as e:
            logging.error(f"AI ë¶„ì„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def get_analysis_statistics(self, date: str) -> Dict:
        """ë¶„ì„ í†µê³„ ì¡°íšŒ"""
        try:
            table_name = f"ai_analysis_{date.replace('-', '')}"

            connection = self.get_connection(self.crawling_db)
            cursor = connection.cursor(pymysql.cursors.DictCursor)

            # ê¸°ë³¸ í†µê³„
            cursor.execute(f"""
            SELECT 
                COUNT(*) as total_count,
                AVG(ai_score) as avg_score,
                AVG(keyword_score) as avg_keyword_score,
                AVG(combo_bonus) as avg_combo_bonus,
                AVG(market_score) as avg_market_score,
                AVG(sustainability_score) as avg_sustainability_score
            FROM {table_name}
            """)

            basic_stats = cursor.fetchone()

            # íˆ¬ì ì˜ê²¬ë³„ í†µê³„
            cursor.execute(f"""
            SELECT investment_opinion, COUNT(*) as count
            FROM {table_name}
            GROUP BY investment_opinion
            ORDER BY count DESC
            """)

            opinion_stats = cursor.fetchall()

            # ì´ìŠˆ íƒ€ì…ë³„ í†µê³„
            cursor.execute(f"""
            SELECT issue_type, COUNT(*) as count, AVG(ai_score) as avg_score
            FROM {table_name}
            GROUP BY issue_type
            """)

            issue_type_stats = cursor.fetchall()

            # ê³ ë“ì  ì¢…ëª© (80ì  ì´ìƒ)
            cursor.execute(f"""
            SELECT stock_name, ai_score, investment_opinion, total_calculated_score
            FROM {table_name}
            WHERE ai_score >= 80
            ORDER BY ai_score DESC
            LIMIT 10
            """)

            high_score_stocks = cursor.fetchall()

            cursor.close()
            connection.close()

            return {
                'basic_stats': basic_stats,
                'opinion_stats': opinion_stats,
                'issue_type_stats': issue_type_stats,
                'high_score_stocks': high_score_stocks
            }

        except Exception as e:
            logging.error(f"ë¶„ì„ í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}

    def check_table_exists(self, table_name: str) -> bool:
        """í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸"""
        try:
            connection = self.get_connection(self.crawling_db)
            cursor = connection.cursor()

            query = """
            SELECT COUNT(*) as count 
            FROM information_schema.tables 
            WHERE table_schema = %s AND table_name = %s
            """

            cursor.execute(query, (self.crawling_db, table_name))
            result = cursor.fetchone()

            cursor.close()
            connection.close()

            return result[0] > 0 if result else False

        except Exception as e:
            logging.error(f"í…Œì´ë¸” ì¡´ì¬ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

    def migrate_existing_table(self, date: str) -> bool:
        """ê¸°ì¡´ í…Œì´ë¸”ì„ ê°œì„ ëœ ìŠ¤í‚¤ë§ˆë¡œ ë§ˆì´ê·¸ë ˆì´ì…˜"""
        try:
            table_name = f"ai_analysis_{date.replace('-', '')}"

            if not self.check_table_exists(table_name):
                logging.info(f"í…Œì´ë¸” {table_name}ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                return False

            connection = self.get_connection(self.crawling_db)
            cursor = connection.cursor()

            # ìƒˆ ì»¬ëŸ¼ë“¤ ì¶”ê°€
            new_columns = [
                "ADD COLUMN keyword_score INT DEFAULT 0",
                "ADD COLUMN combo_bonus INT DEFAULT 0",
                "ADD COLUMN market_score INT DEFAULT 0",
                "ADD COLUMN sustainability_score INT DEFAULT 0",
                "ADD COLUMN total_calculated_score INT DEFAULT 0",
                "ADD COLUMN found_keywords JSON",
                "ADD COLUMN found_categories JSON",
                "ADD COLUMN analysis_summary TEXT",
                "ADD COLUMN updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP"
            ]

            for column_sql in new_columns:
                try:
                    cursor.execute(f"ALTER TABLE {table_name} {column_sql}")
                    logging.info(f"ì»¬ëŸ¼ ì¶”ê°€: {column_sql}")
                except Exception as e:
                    logging.warning(f"ì»¬ëŸ¼ ì¶”ê°€ ì‹¤íŒ¨ (ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ): {e}")

            # ìƒˆ ì¸ë±ìŠ¤ ì¶”ê°€
            new_indexes = [
                f"CREATE INDEX idx_total_score ON {table_name} (total_calculated_score DESC)",
                f"CREATE INDEX idx_keyword_score ON {table_name} (keyword_score DESC)"
            ]

            for index_sql in new_indexes:
                try:
                    cursor.execute(index_sql)
                    logging.info(f"ì¸ë±ìŠ¤ ì¶”ê°€: {index_sql}")
                except Exception as e:
                    logging.warning(f"ì¸ë±ìŠ¤ ì¶”ê°€ ì‹¤íŒ¨ (ì´ë¯¸ ì¡´ì¬í•  ìˆ˜ ìˆìŒ): {e}")

            cursor.close()
            connection.close()

            logging.info(f"âœ… {table_name} í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ")
            return True

        except Exception as e:
            logging.error(f"í…Œì´ë¸” ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹¤íŒ¨: {e}")
            return False


# ê¸°ì¡´ database.pyì˜ TopRateDatabase í´ë˜ìŠ¤ í™•ì¥
class TopRateDatabase:
    """ê¸°ì¡´ TopRateDatabase í´ë˜ìŠ¤ì— ê°œì„ ëœ ë©”ì„œë“œ ì¶”ê°€"""

    def __init__(self):
        self.db_config = {
            'host': os.getenv('DB_HOST', 'localhost'),
            'port': int(os.getenv('DB_PORT', 3306)),
            'user': os.getenv('DB_USER'),
            'password': os.getenv('DB_PASSWORD'),
            'charset': 'utf8mb4',
            'autocommit': True
        }
        self.crawling_db = 'crawling_db'

    def get_connection(self, database: str = None) -> pymysql.Connection:
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ë°˜í™˜"""
        config = self.db_config.copy()
        if database:
            config['database'] = database
        return pymysql.connect(**config)

    def setup_crawling_database(self):
        """í¬ë¡¤ë§ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •"""
        try:
            connection = self.get_connection()
            cursor = connection.cursor()
            cursor.execute(
                f"CREATE DATABASE IF NOT EXISTS {self.crawling_db} CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci")
            cursor.close()
            connection.close()
            return True
        except Exception as e:
            logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False

    def setup_ai_analysis_table(self, date: str) -> str:
        """ê°œì„ ëœ AI ë¶„ì„ í…Œì´ë¸” ì„¤ì • (ê¸°ì¡´ ë©”ì„œë“œë¥¼ ê°œì„ ëœ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´)"""
        enhanced_db = EnhancedTopRateDatabase()
        return enhanced_db.setup_enhanced_ai_analysis_table(date)

    def save_ai_analysis(self, table_name: str, analysis_data: List[Dict]) -> bool:
        """ê°œì„ ëœ AI ë¶„ì„ ê²°ê³¼ ì €ì¥ (ê¸°ì¡´ ë©”ì„œë“œë¥¼ ê°œì„ ëœ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´)"""
        enhanced_db = EnhancedTopRateDatabase()
        return enhanced_db.save_enhanced_ai_analysis(table_name, analysis_data)

    def get_ai_analysis(self, date: str) -> List[Dict]:
        """ê°œì„ ëœ AI ë¶„ì„ ê²°ê³¼ ì¡°íšŒ (ê¸°ì¡´ ë©”ì„œë“œë¥¼ ê°œì„ ëœ ë²„ì „ìœ¼ë¡œ ëŒ€ì²´)"""
        enhanced_db = EnhancedTopRateDatabase()
        return enhanced_db.get_enhanced_ai_analysis(date)

    def get_theme_data(self, date: str) -> List[Dict]:
        """í…Œë§ˆ ë°ì´í„° ì¡°íšŒ (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)"""
        try:
            table_name = f"theme_{date.replace('-', '')}"
            connection = self.get_connection(self.crawling_db)
            cursor = connection.cursor(pymysql.cursors.DictCursor)

            query = f"SELECT * FROM {table_name} ORDER BY change_rate DESC"
            cursor.execute(query)
            result = cursor.fetchall()

            cursor.close()
            connection.close()
            return result if result else []

        except Exception as e:
            logging.error(f"í…Œë§ˆ ë°ì´í„° ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []

    def check_table_exists(self, table_name: str) -> bool:
        """í…Œì´ë¸” ì¡´ì¬ ì—¬ë¶€ í™•ì¸ (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)"""
        enhanced_db = EnhancedTopRateDatabase()
        return enhanced_db.check_table_exists(table_name)